{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    precision_recall_fscore_support, \n",
    "    recall_score,\n",
    "    make_scorer,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import rpy2.robjects as robjects\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.options.display.width = 200\n",
    "pd.options.display.column_space = 36\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "import matplotlib.gridspec as gridspec\n",
    "np.set_printoptions(precision=3)\n",
    "random_state=42\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0);\n",
    "from skorch.classifier import NeuralNetClassifier, NeuralNetBinaryClassifier\n",
    "from skorch.callbacks import EpochScoring, EarlyStopping, Callback\n",
    "\n",
    "\n",
    "class_names = np.array([\"No event\", \"Met event\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring \n",
    "\n",
    "skorch_auc_scoring = EpochScoring(scoring='roc_auc', lower_is_better=False, on_train=True)\n",
    "skorch_recall_scoring = EpochScoring(scoring='recall', lower_is_better=False, on_train=True)\n",
    "\n",
    "skorch_scoring = {'roc_auc': skorch_auc_scoring, 'recall' : skorch_recall_scoring}\n",
    "\n",
    "class Scores():\n",
    "    def __init__(self, dataset=None, labels=None, clf=None):\n",
    "        \n",
    "        if dataset is None or labels is None or clf is None:\n",
    "            self.roc = 0\n",
    "            self.accuracy = 0\n",
    "            self.precision = 0\n",
    "            self.recall = 0\n",
    "            self.f1 = 0\n",
    "            self.cm = None\n",
    "        \n",
    "        else:\n",
    "\n",
    "            _, _, self.roc = calculate_roc_score(clf, dataset, labels)\n",
    "\n",
    "            predictions = np.array(clf.predict(dataset), dtype=np.int32)\n",
    "            labels = np.array(labels, dtype=np.int32)\n",
    "            scores = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "            self.accuracy = accuracy_score(labels, predictions)\n",
    "            self.precision = scores[0]\n",
    "            self.recall = scores[1]\n",
    "            self.f1 = scores[2]\n",
    "            self.cm = confusion_matrix(labels, predictions)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'ROC AUC: {:.3f}\\nPrecision: {:.3f}\\nRecall: {:.3f}\\nF1: {:.3f}'.format(self.roc, self.precision,\n",
    "                                                                                                       self.recall, self.f1)\n",
    "    def __iadd__(self, other):\n",
    "        self.roc += other.roc\n",
    "        self.accuracy += other.accuracy\n",
    "        self.precision += other.precision\n",
    "        self.recall += other.recall\n",
    "        self.f1 += other.f1\n",
    "        return self\n",
    "        \n",
    "    def __itruediv__(self, other):\n",
    "        self.roc /= other\n",
    "        self.accuracy /= other\n",
    "        self.precision /= other\n",
    "        self.recall /= other\n",
    "        self.f1 /= other\n",
    "        return self\n",
    "    \n",
    "def print_scores(train_scores, test_scores, latex=False):\n",
    "    data = [\n",
    "        [train_scores.roc, train_scores.precision, train_scores.recall, train_scores.f1], \n",
    "        [test_scores.roc, test_scores.precision, test_scores.recall, test_scores.f1]\n",
    "    ]\n",
    "    \n",
    "    frame = pd.DataFrame(data, columns=[\"ROC AUC\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    frame.rename({0: 'train', 1:'test'}, axis='index', inplace=True)\n",
    "    display(frame)\n",
    "    if latex:\n",
    "        print(frame.to_latex())\n",
    "        \n",
    "        \n",
    "def print_summarized_scores(estimated_scores, models, latex=False):\n",
    "    summarized_scores = []\n",
    "    for name, scores in estimated_scores.items():\n",
    "        model_summary = []\n",
    "        for score in scores.values():\n",
    "            model_summary += ['{:.3f} $\\pm$ {:.3f}'.format(score['mean'], score['std'])]\n",
    "        summarized_scores += [model_summary]\n",
    "\n",
    "    frame = pd.DataFrame(summarized_scores, columns=[\"ROC AUC\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    frame.rename({i : name for i, name in enumerate(models.keys())}, axis='index', inplace=True)\n",
    "    display(frame)\n",
    "    if latex:\n",
    "        print(frame.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(name):\n",
    "    return np.genfromtxt(name, delimiter=\",\", skip_header=1)\n",
    "\n",
    "\n",
    "def load_train_and_test_parts():\n",
    "    X_train = load_file(\"data/microarray_train.csv\")\n",
    "    X_test = load_file(\"data/microarray_test.csv\")\n",
    "    y_train = load_file(\"data/labels_train.csv\")\n",
    "    y_test = load_file(\"data/labels_test.csv\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def flatten(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "\n",
    "def fit_models(\n",
    "    train_set, train_labels, test_set, test_labels, plot_logit_weigths=False\n",
    "):\n",
    "    clf_logit = fit_clf(\n",
    "        LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=0.3),\n",
    "        train_set,\n",
    "        train_labels,\n",
    "        test_set,\n",
    "        test_labels,\n",
    "        \"Logistics regression\",\n",
    "    )\n",
    "    if plot_logit_weigths:\n",
    "        plot_logit_weights(clf_logit, \"Logistics regression coefficients\")\n",
    "    # clf_svm = fit_clf(SVC(gamma='scale', C=7, probability=True), train_set, train_labels, test_set, test_labels, 'SVM')\n",
    "    clf_forest = fit_clf(\n",
    "        RandomForestClassifier(max_depth=4, n_estimators=2000, min_samples_leaf=10),\n",
    "        train_set,\n",
    "        train_labels,\n",
    "        test_set,\n",
    "        test_labels,\n",
    "        \"Random forest\",\n",
    "    )\n",
    "    return (clf_logit, clf_forest)\n",
    "\n",
    "def fit_clf_print_scores(clf, train_set, train_labels, test_set, test_labels):\n",
    "    clf, train_scores, test_scores = fit_clf_scores(clf, train_set, train_labels, test_set, test_labels)\n",
    "    return print_after_fit(clf, train_scores, test_scores)\n",
    "\n",
    "def clf_print_scores(clf, train_set, train_labels, test_set, test_labels):\n",
    "    clf, train_scores, test_scores = clf_scores(clf, train_set, train_labels, test_set, test_labels)\n",
    "    return print_after_fit(clf, train_scores, test_scores)\n",
    "\n",
    "def fit_clf(clf, train_set, train_labels, test_set, test_labels, title):\n",
    "    clf = clf.fit(train_set, train_labels)\n",
    "    plot_clf_roc(clf, train_set, train_labels, test_set, test_labels, title)\n",
    "    return clf\n",
    "\n",
    "def fit_clf_scores(clf, train_set, train_labels, test_set, test_labels):\n",
    "    clf = clf.fit(train_set, train_labels)\n",
    "    return clf_scores(clf, train_set, train_labels, test_set, test_labels)\n",
    "\n",
    "def clf_scores(clf, train_set, train_labels, test_set, test_labels):\n",
    "    train_scores = Scores(train_set, train_labels, clf)\n",
    "    test_scores = Scores(test_set, test_labels, clf)\n",
    "    return clf, train_scores, test_scores\n",
    "\n",
    "def print_after_fit(clf, train_scores, test_scores):\n",
    "    if hasattr(clf, 'best_params_'):\n",
    "        print(clf.best_params_)\n",
    "    print_scores(train_scores, test_scores)\n",
    "    return clf\n",
    "    \n",
    "# Cross validation\n",
    "\n",
    "def fit_clf_with_cross_val(clf, train_set, train_labels, test_set, test_labels, title):\n",
    "    fig, (ax1, ax2) = subplots(\n",
    "        nrows=1, ncols=2, sharex=True, sharey=True, figsize=(16, 8)\n",
    "    )\n",
    "    fig.suptitle(title)\n",
    "    fit_clf_cv(ax1, clf, train_set, train_labels, title=\"Train\")\n",
    "    plot_roc_curve(ax2, clf, test_set, test_labels, title=\"Test\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fit_clf_cv(ax, clf, X, y, title):\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    accs = [] \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    i = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        model = clf.fit(X[train], y[train])\n",
    "        probas_ = model.predict_proba(X[test])\n",
    "        acc = model.score(X[test], y[test])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        accs.append(acc)\n",
    "        ax.plot(\n",
    "            fpr, tpr, lw=1, alpha=0.3, label=\"ROC fold %d (AUC = %0.3f)\" % (i, roc_auc)\n",
    "        )\n",
    "\n",
    "        i += 1\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.3f $\\pm$ %0.3f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlim([-0.05, 1.05])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(title + r\" ( Accuracy = %0.3f $\\pm$ %0.3f )\" % (np.mean(accs), np.std(accs)))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "# ROC\n",
    "\n",
    "\n",
    "def calculate_roc_score(clf, dataset, labels):\n",
    "    \n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        probs = clf.predict_proba(dataset)\n",
    "        if len(probs.shape) > 1:\n",
    "            probs = probs[:, 1]\n",
    "    else:\n",
    "        probs = clf.decision_function(dataset)\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "def plot_roc_curve(ax, clf, dataset, labels, title):\n",
    "\n",
    "    fpr, tpr, roc_auc = calculate_roc_score(clf, dataset, labels)\n",
    "\n",
    "    plot_roc(ax, fpr, tpr, roc_auc, title)\n",
    "    \n",
    "def plot_roc(ax, fpr, tpr, roc_auc, title):\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "    ax.plot(fpr, tpr, label=\"ROC curve (AUC = {:.3f})\".format(roc_auc))\n",
    "    ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=title,\n",
    "    )\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def plot_clf_roc(clf, train_set, train_labels, test_set, test_labels, title):\n",
    "    fig, (ax1, ax2) = subplots(\n",
    "        nrows=1, ncols=2, sharex=True, sharey=True, figsize=(12, 6)\n",
    "    )\n",
    "    fig.suptitle(title)\n",
    "    plot_roc_curve(ax1, clf, train_set, train_labels, title=\"Train\")\n",
    "    plot_roc_curve(ax2, clf, test_set, test_labels, title=\"Test\")\n",
    "    plt.show()\n",
    "\n",
    "# Logit wieghts\n",
    "\n",
    "def plot_logit_weights(clf_logit, title):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(clf_logit.coef_.shape[1]), clf_logit.coef_[0])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_logit_weights_ax(ax, clf_logit, title):\n",
    "    ax.set_title(title)\n",
    "    sns.lineplot(np.arange(clf_logit.coef_.shape[1]), clf_logit.coef_[0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stubTransformer = FunctionTransformer(lambda x : x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting threshold\n",
    "\n",
    "def adjusted_classes(scores, t):\n",
    "    return [1 if y >= t else 0 for y in scores]\n",
    "\n",
    "def plot_precision_recall_vs_threshold(ax, labels, scores, clf_name, requsted=[0.9, 0.8, 0.7]):\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(labels, scores)\n",
    "    print(clf_name + ':')\n",
    "    for thresh in requsted:\n",
    "        index = np.argmin(recalls >= thresh) - 1\n",
    "        print('For threshold {:.3f} recall is {:.3f} and precision is {:.3f}'.format(thresholds[index], recalls[index], precisions[index]))\n",
    "    \n",
    "    ax.set_title('Precision and recall for ' + clf_name)\n",
    "    ax.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    ax.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xlabel(\"Decision Threshold\")\n",
    "    ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLCC functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlcc_result_files():\n",
    "    filenames = []\n",
    "    for filename in os.listdir('./mlcc_results/'):\n",
    "        if filename.endswith('.RData'):\n",
    "            filenames.append(filename)\n",
    "    return filenames\n",
    "    \n",
    "\n",
    "\n",
    "class MLCCWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=726, filename=''):\n",
    "        self.size = size\n",
    "        self.filename = filename\n",
    "        self.PCAs = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.segmentation, self.dimensionalities = self.get_result() \n",
    "        numb_clust = self.dimensionalities.shape[0]\n",
    "        \n",
    "        for i in range(numb_clust):\n",
    "            cluster = X[:, self.segmentation == i]\n",
    "            n_components = self.dimensionalities[i]\n",
    "            if cluster.shape[1] < n_components:  # TODO - maybe mlcc shouldn't allow it\n",
    "                print(\n",
    "                    \"WARNING! Dimensionality of a cluster was greater than the number of variables. Ignoring this cluster.\"\n",
    "                )\n",
    "            else:\n",
    "                self.PCAs += [PCA(n_components=n_components).fit(cluster)]\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_reduced = np.empty((X.shape[0], 0))\n",
    "        numb_clust = self.dimensionalities.shape[0]\n",
    "        \n",
    "        for i in range(numb_clust):\n",
    "            cluster = X[:, self.segmentation == i]\n",
    "            X_reduced = np.concatenate(\n",
    "                        (X_reduced, self.PCAs[i].transform(cluster)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "        return X_reduced\n",
    "        \n",
    "    \n",
    "    def get_result(self):\n",
    "        self.mlcc_result = tuple(self.read_mlcc_result(self.filename, self.size))\n",
    "                \n",
    "        self.mBIC = self.mlcc_result[1][0]\n",
    "        \n",
    "        segmentation = self.mlcc_result[0]\n",
    "        dimensionalities = self.mlcc_result[2]\n",
    "        return segmentation, dimensionalities\n",
    "        \n",
    "        \n",
    "    def summary(self, best_index=None):\n",
    "        filenames = get_mlcc_result_files()\n",
    "        print(filenames)\n",
    "        mlcc_results = []\n",
    "        for filename in filenames:\n",
    "            mlcc_results.append(tuple(self.read_mlcc_result(filename, self.size)))\n",
    "                \n",
    "        mBICs = list(map(lambda x : x[1][0], mlcc_results))\n",
    "        max_index = np.argmax(mBICs)\n",
    "        print('Highest mBIC is from {}'.format(filenames[max_index]))\n",
    "        figure()\n",
    "        lengthMBICs = len(mBICs)\n",
    "        if best_index is not None:\n",
    "            scatter(np.arange(lengthMBICs)[0:best_index], mBICs[0:best_index], c='b')\n",
    "            scatter(np.arange(lengthMBICs)[best_index], mBICs[best_index], c='r')\n",
    "            if best_index < lengthMBICs:\n",
    "                scatter(np.arange(lengthMBICs)[(best_index+1):], mBICs[(best_index+1):], c='b')\n",
    "            print(self.segmentation, self.dimensionalities)\n",
    "        else:\n",
    "            scatter(np.arange(lengthMBICs), mBICs)\n",
    "        show()\n",
    "\n",
    "    def read_mlcc_result(self, filename, train_size):\n",
    "        robjects.r[\"load\"](\"./mlcc_results/{}\".format(filename))\n",
    "        s, m, b = robjects.r[\"res\"]\n",
    "        segmentation = np.asarray(s)\n",
    "        numb_clust = np.max(s)\n",
    "        mBIC = np.asarray(m)\n",
    "        b.names = robjects.r(\"0:{}\".format(numb_clust - 1))\n",
    "        bases = dict(zip(b.names, map(list, list(b))))\n",
    "        dimensionalities = np.empty(numb_clust, dtype=np.int32)\n",
    "        for i in range(numb_clust):\n",
    "            dimensionalities[i] = len(bases[str(i)]) // train_size\n",
    "        return segmentation - 1, mBIC, dimensionalities\n",
    "\n",
    "\n",
    "    def apply_mlcc_dim_reduction(self, X, segmentation, dimensionalities):\n",
    "        numb_clust = dimensionalities.shape[0]\n",
    "        X_reduced = np.empty((X.shape[0], 0))\n",
    "        for i in range(numb_clust):\n",
    "            cluster = X[:, segmentation == i]\n",
    "            n_components = dimensionalities[i]\n",
    "            if cluster.shape[1] < n_components:  # TODO - maybe mlcc shouldn't allow it\n",
    "                print(\n",
    "                    \"WARNING! Dimensionality of a cluster was greater than the number of variables. Ignoring this cluster.\"\n",
    "                )\n",
    "            else:\n",
    "                X_reduced = np.concatenate(\n",
    "                    (X_reduced, PCA(n_components=n_components).fit_transform(cluster)),\n",
    "                    axis=1,\n",
    "                )\n",
    "        return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Logistic Regression (like Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLogisticsRegressions(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=11,\n",
    "        penalty=\"l2\",\n",
    "        tol=1e-4,\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\",\n",
    "        n_variables=1000\n",
    "    ):\n",
    "        self.penalty = penalty\n",
    "        self.tol = tol\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_variables = n_variables\n",
    "        \n",
    "        self.estimators_ = []\n",
    "        self.indices = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.estimators_ = [\n",
    "            LogisticRegression(penalty=self.penalty, tol=self.tol, C=self.C, solver=self.solver)\n",
    "            for x in np.arange(self.n_estimators)\n",
    "        ]\n",
    "        \n",
    "        self.indices = np.array(\n",
    "            [\n",
    "                np.random.choice(\n",
    "                    np.arange(X.shape[1]), size=self.n_variables, replace=False\n",
    "                )\n",
    "                for x in np.arange(self.n_estimators)\n",
    "            ]\n",
    "        )\n",
    "        for i in np.arange(self.n_estimators):\n",
    "            self.estimators_[i].fit(X[:, self.indices[i, :]], y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        models_predictions = np.array(\n",
    "            [\n",
    "                model.predict(X[:, self.indices[i, :]])\n",
    "                for i, model in enumerate(self.estimators_)\n",
    "            ]\n",
    "        )\n",
    "        mean_predictions = np.mean(models_predictions, axis=0)\n",
    "        return np.round(mean_predictions)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        models_probs = np.array(\n",
    "            [\n",
    "                model.predict_proba(X[:, self.indices[i, :]])\n",
    "                for i, model in enumerate(self.estimators_)\n",
    "            ]\n",
    "        )\n",
    "        probabilities = np.mean(models_probs, axis=0)\n",
    "        return probabilities\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for RLR and RandomForest\n",
    "\n",
    "class RepresentationTransformer(BaseEstimator, TransformerMixin):   \n",
    "    def __init__(self, transformer, n_components = 2, probabilistic=True):\n",
    "        self.transformer = transformer\n",
    "        self.probabilistic = probabilistic\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.transformer.set_params(**{'n_estimators' : self.n_components})\n",
    "        #reset transformer between fits\n",
    "        self.transformer = clone(self.transformer)\n",
    "        self.transformer.fit(X,y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if self.probabilistic:\n",
    "            return self.get_probabilistic_representation(X)\n",
    "        else:\n",
    "            return self.get_representation(X)\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_representation(self, data):\n",
    "        result = np.zeros((data.shape[0], len(self.transformer.estimators_)), dtype=np.int32)\n",
    "        for i, estimator in enumerate(self.transformer.estimators_):\n",
    "            result[:, i] = estimator.predict(data[:, self.get_indices(i, data)])\n",
    "        return result\n",
    "\n",
    "    def get_probabilistic_representation(self, data):\n",
    "        result = np.zeros((data.shape[0], len(self.transformer.estimators_)), dtype=np.float32)\n",
    "        for i, estimator in enumerate(self.transformer.estimators_):\n",
    "            result[:, i] = estimator.predict_proba(data[:, self.get_indices(i, data)])[:, 1]\n",
    "        return result\n",
    "\n",
    "    def get_indices(self, i, data):\n",
    "        if isinstance(self.transformer, RandomForestClassifier):\n",
    "            return np.arange(data.shape[1])\n",
    "        try:\n",
    "            # assuming RLR type (hack because isinstance is not working here properly due to different modules)\n",
    "            return self.transformer.indices[i, :]\n",
    "        except:\n",
    "            raise NotImplementedError(\"For this type it is not yet implemented: \" +  str(type(self.transformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM transformer\n",
    "\n",
    "def get_gene_names():\n",
    "    dataset = pd.read_csv('data/microarray_data.csv', delimiter=\",\", index_col=0)\n",
    "    gene_names = list(dataset.index.values)\n",
    "    return gene_names\n",
    "\n",
    "def get_sam_result_files():\n",
    "    filenames = []\n",
    "    for filename in os.listdir('./sam_results/'):\n",
    "        if filename.endswith('.csv'):\n",
    "            filenames.append(filename)\n",
    "    return filenames\n",
    "    \n",
    "\n",
    "class SAMSelection(BaseEstimator, TransformerMixin):   \n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "        self.gene_names = get_gene_names()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        significant_genes = np.genfromtxt(\"./sam_results/{}\".format(self.filename), skip_header=1, delimiter=',', dtype=np.str)\n",
    "        significant_genes = [x.replace('\"', '') for x in significant_genes]\n",
    "        self.significant_genes_indices = [self.gene_names.index(x) for x in significant_genes]\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X[:, self.significant_genes_indices]\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net\n",
    "\n",
    "class NeuralNetClassifierWrapper(NeuralNetClassifier):\n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y, dtype=np.int64)\n",
    "        return super().fit(X,y)\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_hidden0=10,\n",
    "            num_hidden1=10,\n",
    "            dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_hidden0 = num_hidden0\n",
    "        self.num_hidden1 = num_hidden1\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.input_initialized = False\n",
    "        self.dense0 = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense1 = nn.Linear(num_hidden0, num_hidden1)\n",
    "        self.output = nn.Linear(num_hidden1, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        \n",
    "        if not self.input_initialized:\n",
    "            self.dense0 = nn.Linear(X.shape[-1], self.num_hidden0)\n",
    "            self.input_initialized = True\n",
    "            \n",
    "        X = F.relu(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "\n",
    "# Wrapper for SPC from easyspc package - function transform doesn't use k parameter anymore (n_components instead), renamed number\n",
    "# of components to n_components, and changed default of threshold_val, got rid of prints\n",
    "\n",
    "class SPCWrapper(BaseEstimator):\n",
    "    def __init__(self, n_components=2, max_iter=50, threshold_val=10, threshold_mode='soft'):\n",
    "    \n",
    "        self.n_components=n_components\n",
    "        self.max_iter=max_iter\n",
    "        self.threshold_val=threshold_val\n",
    "        self.threshold_mode = threshold_mode\n",
    "    \n",
    "    def fit(self, X_, y=None):\n",
    "        \n",
    "        U, s, V = np.linalg.svd(X_, full_matrices=True)  \n",
    "        cnt = 0\n",
    "        self.U = U\n",
    "        self.W=V.T\n",
    "        def normalize(vector):\n",
    "            norm=np.linalg.norm(vector)\n",
    "            if norm>0:\n",
    "                return vector/norm\n",
    "            else:\n",
    "                return vector\n",
    "            \n",
    "        for i in range(self.max_iter):\n",
    "            self.V = pywt.threshold(np.dot(U[:self.n_components],X_), self.threshold_val, mode=self.threshold_mode)\n",
    "            self.U = np.dot(self.V,X_.T)\n",
    "            self.U = np.array([normalize(u_i) for u_i in self.U])\n",
    "        self.V = np.array([normalize(v_i) for v_i in self.V])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_):\n",
    "        X_reduced_spca = np.dot(X_, np.dot(self.V[:self.n_components].T, self.V[:self.n_components]))\n",
    "        return X_reduced_spca\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSRegressionWrapper(PLSRegression):\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return super().transform(X)\n",
    "    \n",
    "    def fit_transform(self, X, Y):\n",
    "        return self.fit(X,Y).transform(X)\n",
    "    \n",
    "class NearestCentroidWrapper(NearestCentroid):\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_REDUCTION = 'dim'\n",
    "CLASSIFIER='clf'\n",
    "STANDARDIZER='std'\n",
    "\n",
    "def get_dim_reduction_pipeline(dim_reduction, dim_reduction_params, standardizer=StandardScaler(), \n",
    "                               clf=LogisticRegression(random_state=random_state),\n",
    "                              clf_params={'C' : np.linspace(0.1, 1, 50), 'penalty' : ['l1'], 'solver' : ['liblinear']}):\n",
    "    param_grid = transform_params(dim_reduction_params, clf_params)\n",
    "    pipeline = Pipeline(steps=[(STANDARDIZER, StandardScaler()), (DIM_REDUCTION, dim_reduction), (CLASSIFIER, clf)])\n",
    "    return pipeline, param_grid\n",
    "\n",
    "\n",
    "def fit_dim_reduction_pipeline(dim_reduction, dim_reduction_params, X, y, standardizer=StandardScaler(), \n",
    "                               clf=LogisticRegression(random_state=random_state),\n",
    "                              clf_params={'C' : np.linspace(0.1, 1, 50), 'penalty' : ['l1'], 'solver' : ['liblinear']}, \n",
    "                               n_iter=50, cv=4, scoring='roc_auc', n_jobs=3):\n",
    "    \n",
    "    pipeline, param_grid = get_dim_reduction_pipeline(dim_reduction, dim_reduction_params, standardizer, clf, clf_params)\n",
    "    \n",
    "    # fortunately, according to https://stackoverflow.com/questions/14955458/does-gridsearchcv-use-predict-or-predict-proba-when-using-auc-score-as-score-fu,\n",
    "    # scoring='roc_auc implies using predict_proba'\n",
    "    search = RandomizedSearchCV(pipeline, cv=cv, scoring=scoring, n_iter=n_iter, n_jobs=n_jobs, random_state=random_state,\n",
    "                                iid=False, param_distributions=param_grid)\n",
    "    \n",
    "    search.fit(X, y)\n",
    "    \n",
    "    return search\n",
    "\n",
    "\n",
    "def transform_params(dim_reduction_params, clf_params):\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for key, value in dim_reduction_params.items():\n",
    "        params[DIM_REDUCTION + '__' + key] = value\n",
    "        \n",
    "    for key, value in clf_params.items():\n",
    "        params[CLASSIFIER + '__' + key] = value \n",
    "    \n",
    "    return params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_hist(dataset, column_idx, ax_box, ax_hist):\n",
    "    column_name = dataset.columns[column_idx]\n",
    "    sns.despine(ax=ax_box, left=True)\n",
    "    sns.boxplot(dataset[column_name], ax=ax_box)\n",
    "    ax_box.set(yticks=[])\n",
    "    ax_box.set(xticks=[])\n",
    "    ax_box.set(xlabel='')\n",
    "    ax_hist.get_shared_x_axes().join(ax_box, ax_hist)\n",
    "    sns.despine(ax=ax_hist)\n",
    "    sns.distplot(dataset[column_name], ax=ax_hist)\n",
    "    ax_hist.set(xlabel='{} - {}'.format(column_idx, column_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice functions, maybe use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clf_cm(clf, train_set, train_labels, test_set, test_labels):\n",
    "    test_labels_pred = clf.predict(test_set)\n",
    "    train_labels_pred = clf.predict(train_set)\n",
    "    test_cm = confusion_matrix(test_labels, test_labels_pred)\n",
    "    train_cm = confusion_matrix(train_labels, train_labels_pred)\n",
    "    fig, (ax1, ax2) = subplots(nrows=1, ncols=2, sharex=True, sharey=True)\n",
    "    im = plot_confusion_matrix(\n",
    "        ax1,\n",
    "        train_cm,\n",
    "        classes=class_names,\n",
    "        normalize=True,\n",
    "        title=\"Train confusion matrix\",\n",
    "    )\n",
    "    im = plot_confusion_matrix(\n",
    "        ax2, test_cm, classes=class_names, normalize=True, title=\"Test confusion matrix\"\n",
    "    )\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    show()\n",
    "    \n",
    "def plot_confusion_matrix(\n",
    "    axis, cm, classes, normalize=False, title=\"Confusion matrix\", cmap=cm.Blues\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    im = axis.imshow(cm, interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n",
    "    axis.set(title=title, xlabel=\"Predicted label\", ylabel=\"True label\")\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    axis.set_xticks(tick_marks)\n",
    "    axis.set_xticklabels(classes)\n",
    "    axis.set_yticks(tick_marks)\n",
    "    axis.set_yticklabels(classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        axis.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    return im"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
